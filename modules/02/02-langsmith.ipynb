{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixSi6udtrYZk"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/02-langsmith.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NwdBTO0qCQi"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swum099VqCQj"
      },
      "source": [
        "# LangSmith Starter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hk437ZjqCQj"
      },
      "source": [
        "LangSmith is a built-in observability service and platform that integrates _very easily_ with LangChain. You don't _need_ to use LangSmith for this course, but it can be very helpful in understanding what is happening, _and_ we recommend using it beyond this course for general development with LangChain — with all of that in mind we would recommend spending a little bit of time to get familiar with LangSmith."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UdyCjmqqCQk"
      },
      "source": [
        "## Setting up LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqURpTWbqCQk"
      },
      "source": [
        "LangSmith does require an API key, but it comes with a generous free tier. You can sign up an account and get your API key [here](https://smith.langchain.com).\n",
        "\n",
        "When using LangSmith, we need to setup our environment variables _and_ provide our API key, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezoLeIGrqCQk",
        "outputId": "252aa252-d8c3-4cd5-fb18-acf3cad2dee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()  # take environment variables from .env file\n",
        "\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Abhilasha-Test-Project\"\n",
        "print(os.getenv(\"LANGCHAIN_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGDntgWqCQk"
      },
      "source": [
        "In most cases, this is all we need to start seeing logs and traces in the [LangSmith UI](https://smith.langchain.com). By default, LangChain will trace LLM calls, chains, etc. We'll take a look at a quick example of this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFDeEk0tqCQk"
      },
      "source": [
        "## Default Tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3hqGO2qCQk"
      },
      "source": [
        "As mentioned, LangSmith traces a lot of data without us needing to do anything. Let's see how that looks. We'll start by initializing our LLM. Again, this will need an [API key](https://platform.openai.com/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AfToUsEqCQl",
        "outputId": "4426952f-af46-440d-d275-a86610a9c1fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
        "    \"Enter Gemini API Key: \"\n",
        ")\n",
        "\n",
        "gemini_model = \"gemini-2.5-flash-lite\"  # or \"gemini-2.5-flash\" for more power\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatGoogleGenerativeAI(temperature=0.0, \n",
        "                             model=gemini_model, \n",
        "                             google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "                             max_retries=2,\n",
        "                             timeout=None,\n",
        "                             max_tokens=None)\n",
        "\n",
        "# For unique creative responses\n",
        "creative_llm = ChatGoogleGenerativeAI(temperature=0.9, \n",
        "                                      model=gemini_model, \n",
        "                                      google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "                                      max_retries=2,\n",
        "                                      timeout=None,\n",
        "                                      max_tokens=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVDqCttqCQl"
      },
      "source": [
        "Let's invoke our LLM and then see what happens in the LangSmith UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCGtll-IqCQl",
        "outputId": "0f9ea347-b50f-481c-c237-fd1fd6a09589"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hi Abhilasha! It\\'s great to meet you. I\\'m a large language model, trained by Google.\\n\\nThink of me as a versatile AI assistant. I can help you with a wide range of tasks, including:\\n\\n**Information & Knowledge:**\\n\\n*   **Answering questions:** I can access and process information from the real world and provide you with answers on almost any topic.\\n*   **Explaining concepts:** If you\\'re struggling to understand something, I can break it down into simpler terms.\\n*   **Summarizing text:** I can condense long articles, documents, or web pages into concise summaries.\\n*   **Researching topics:** I can help you gather information on specific subjects.\\n\\n**Creative & Writing Tasks:**\\n\\n*   **Writing different kinds of creative content:** This includes poems, code, scripts, musical pieces, email, letters, etc.\\n*   **Generating ideas:** If you\\'re stuck for inspiration, I can brainstorm ideas for stories, projects, or anything else.\\n*   **Drafting emails and letters:** I can help you compose professional or personal correspondence.\\n*   **Translating languages:** I can translate text between many different languages.\\n*   **Rewriting and improving text:** I can help you refine your writing for clarity, tone, or style.\\n\\n**Problem Solving & Assistance:**\\n\\n*   **Brainstorming solutions:** I can help you think through problems and come up with potential solutions.\\n*   **Providing step-by-step instructions:** If you need to learn how to do something, I can guide you through the process.\\n*   **Generating lists:** I can create lists of items, ideas, or anything else you need.\\n\\n**And much more!**\\n\\n**To give you the best help, tell me what you\\'re looking for! For example, you could ask me to:**\\n\\n*   \"Tell me about the history of the internet.\"\\n*   \"Write a short poem about a rainy day.\"\\n*   \"Help me brainstorm ideas for a birthday gift for my friend.\"\\n*   \"Explain how photosynthesis works.\"\\n*   \"Translate \\'Hello, how are you?\\' into Spanish.\"\\n\\n**So, Abhilasha, what\\'s on your mind today? What can I help you with?**', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--81264946-0506-4be3-bd42-27bd9e606b39-0', usage_metadata={'input_tokens': 16, 'output_tokens': 480, 'total_tokens': 496, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"Hii this side Abhilasha ! what can you do for me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYM-pIIaqCQl"
      },
      "source": [
        "After this we should see that a new project (`aurelioai-langchain-course-langsmith-openai`) has been created in the LangSmith UI. Inside that project, we should see the trace from our LLM call:\n",
        "\n",
        "\n",
        "![LangSmith LLM Trace](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-01.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7VktC46qCQl"
      },
      "source": [
        "By default, LangSmith will capture plenty — however, it won't capture functions from outside of LangChain. Let's see how we can trace those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNQRKbp9qCQl"
      },
      "source": [
        "## Tracing Non-LangChain Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbYn3yY7qCQl"
      },
      "source": [
        "LangSmith can trace functions that are not part of LangChain, we just need to add the `@traceable` decorator. Let's try this for a few simple functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NVo50uq6qCQl"
      },
      "outputs": [],
      "source": [
        "from langsmith import traceable\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "@traceable\n",
        "def generate_random_number():\n",
        "    return random.randint(0, 100)\n",
        "\n",
        "@traceable\n",
        "def generate_string_delay(input_str: str):\n",
        "    number = random.randint(1, 5)\n",
        "    time.sleep(number)\n",
        "    return f\"{input_str} ({number})\"\n",
        "\n",
        "@traceable\n",
        "def random_error():\n",
        "    number = random.randint(0, 1)\n",
        "    if number == 0:\n",
        "        raise ValueError(\"Random error\")\n",
        "    else:\n",
        "        return \"No error\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4b1yJtqCQl"
      },
      "source": [
        "Let's run these a few times and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2af699bd631846bdb2e064981a48ae67",
            "1507edeb2bcf43238ed2431731b51f24",
            "76c4884af2ed4474a07a6f8964cd7b10",
            "302271efa18349fb8fd20e2fa922f17f",
            "59d44325b22241a0828dd1df4c87eb46",
            "fb6ca9c9ad144bc3947e454cc3f3ab77",
            "1e91ab27a61c4530ac1dd2e05a4455ae",
            "a087fea20eb040469312b40081517d67",
            "ba1df7e62b114c6d9062c65a04bb349d",
            "e7967f66202744d1b41ec3ae02a45617",
            "fe1edcf8e1fa4ae2bdae23d2235395e0"
          ]
        },
        "id": "iTDAR7b4qCQl",
        "outputId": "084ca65c-2497-41cf-e6f6-5b946163b5f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anushkaarora/Desktop/LangchainLearning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:27<00:00,  2.73s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for _ in tqdm(range(10)):\n",
        "    generate_random_number()\n",
        "    generate_string_delay(\"Hello\")\n",
        "    try:\n",
        "        random_error()\n",
        "    except ValueError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCoeNfRiqCQl"
      },
      "source": [
        "Those traces should now be visible in the LangSmith UI, again under the same project:\n",
        "\n",
        "![LangSmith Traces](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-02.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuze3LTqCQl"
      },
      "source": [
        "We can various metrics here for each run. First, ofcourse, the run name. We can see any inputs and outputs from each run, we can see if the run raised any errors, it's start time, and latency. Inside the UI we can also filter for specific runs, like so:\n",
        "\n",
        "![LangSmith UI filters](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-03.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7UR0DPwqCQl"
      },
      "source": [
        "There are various other things we can do within the UI, but we'll leave that for you to explore.\n",
        "\n",
        "Finally, we can also modify our traceable names if we'd like to make them more readable inside the UI. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dQLkwo0PqCQl"
      },
      "outputs": [],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "@traceable(name=\"Chitchat Maker\")\n",
        "def error_generation_function(question: str):\n",
        "    delay = random.randint(0, 3)\n",
        "    time.sleep(delay)\n",
        "    number = random.randint(0, 1)\n",
        "    if number == 0:\n",
        "        raise ValueError(\"Random error\")\n",
        "    else:\n",
        "        return \"I'm great how are you?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8NFR6qqCQl"
      },
      "source": [
        "Let's run this a few times and see what we get in LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e988534598c54261a671bc9e17fdb81b",
            "4c3b5008fb254259bfded2cffaa43790",
            "5e63efa23bd54da5bc94a6776f11d0e7",
            "15c2401e19a34ce48286460dad8bf4b2",
            "c884ebe0266b47aa9620600ed2169c81",
            "a27c7c05a70041fcb8dfab45692ef7fc",
            "c1bc2c068ee840d38a4b615f108e4e0c",
            "ffce1b8c4b8646c2b00f81058cb6e6c0",
            "786d6b07a8104f5fa499fbf345bc831f",
            "479c419c88a3443b8c39767fb3540c8b",
            "290f779d73ca42c9bd826518d686bbea"
          ]
        },
        "id": "ay8sxvXWqCQm",
        "outputId": "bafa13da-6ffe-48cb-fa5c-4e18e65a6e3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:20<00:00,  2.01s/it]\n"
          ]
        }
      ],
      "source": [
        "for _ in tqdm(range(10)):\n",
        "    try:\n",
        "        error_generation_function(\"How are you today?\")\n",
        "    except ValueError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsITHuXBqCQm"
      },
      "source": [
        "Let's filter for the `Chitchat Maker` traceable and see our results.\n",
        "\n",
        "![LangSmith Chitchat Maker runs](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-04.jpg?raw=1)\n",
        "\n",
        "We can see our runs and their related metadata! That's it for this intro to LangSmith. As we work through the course we will (optionally) refer to LangSmith for digging into our runs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchainlearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
